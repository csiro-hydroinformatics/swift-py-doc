{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Calibrating tied meta parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## About this document\n",
    "\n",
    "This document illustrates how to set up a calibration where a global parameterization is set at the catchment level, with scaled values for each subareas. This method helps to keep the degrees of freedom of an optimisation to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from swift2.doc_helper import pkg_versions_info\n",
    "\n",
    "print(pkg_versions_info(\"This document was generated from a jupyter notebook\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Use case and sample data\n",
    "\n",
    "This workflow uses for convenience hourly time series data gathered a decade ago. The data comes from the Ovens River catchment, however the provenance is unclear\n",
    "\n",
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cinterop.timeseries import as_timestamp, xr_ts_end\n",
    "from swift2.doc_helper import (\n",
    "    configure_daily_gr4j,\n",
    "    configure_hourly_gr4j,\n",
    "    create_test_catchment_structure,\n",
    "    get_free_params,\n",
    "    gr4j_scaled_parameteriser,\n",
    "    sample_series,\n",
    ")\n",
    "from swift2.parameteriser import (\n",
    "    create_parameter_sampler,\n",
    "    create_sce_optim_swift,\n",
    "    get_default_sce_parameters,\n",
    "    get_marginal_termination,\n",
    "    set_calibration_logger,\n",
    ")\n",
    "from swift2.simulation import get_state_value, get_subarea_ids, swap_model\n",
    "from swift2.utils import as_xarray_series, mk_full_data_id, paste\n",
    "from swift2.vis import OptimisationPlots, plot_two_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The sample data for this tutorial are daily series for the Ovens Catchment in Victoria. Daily streamflow was sourced from https://data.water.vic.gov.au/ at Bright (VIC), and rainfall and morton PET were sourced from  https://www.longpaddock.qld.gov.au/silo/point-data/  at Eurobin (VIC). Note that Eurobin is a bit downstream from Bright and perhaps not the \"best\" point climate data, but this works for this present vignette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_key = \"Ovens-Bright\"\n",
    "daily_rain = sample_series(loc_key, \"rain\")\n",
    "daily_pet = sample_series(loc_key, \"pet\")\n",
    "daily_streamflow_mlday = sample_series(loc_key, \"streamflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_streamflow_mlday.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_area = 495\n",
    "daily_runoff = daily_streamflow_mlday * 1000 * 1000 / (catchment_area * 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_runoff.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cumecs = daily_streamflow_mlday * 1000 / 86400 # ML/day to m3/s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Creating a synthetic but realistic model\n",
    "\n",
    "We create a system with total area similar to the real use case, but we use an arbitrary test catchment structure (multiple subareas). This is suitable for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_portions = np.array([91, 95, 6, 128, 93]) # arbitrary weights.\n",
    "areas_portions = areas_portions / sum(areas_portions)\n",
    "areasKm2 = areas_portions * catchment_area\n",
    "sum(areasKm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, ms = create_test_catchment_structure(areas_km2=areasKm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(areasKm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## channel routing\n",
    "\n",
    "This is besides the main point of this tutorial, but let's take a detour showing how to set up a uniform channel routing using a pure lag routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = swap_model(ms, \"PureLag\", \"channel_routing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "`PureLag` has a `Tau` parameter that can be a positive floating point value. If we query the variable identifiers of one of the links now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.get_variable_ids('link.lnk1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swift2.parameteriser import create_parameteriser\n",
    "p = pd.DataFrame.from_dict({\n",
    "    \"Name\": [\"Tau\"],\n",
    "    \"Value\": [0.25],\n",
    "    \"Min\": [0.25],\n",
    "    \"Max\": [0.25],\n",
    "})\n",
    "pure_lag_six_hours = create_parameteriser('generic links', specs=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.get_state_value('link.lnk1.Tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_lag_six_hours.apply_sys_config(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.get_state_value('link.lnk1.Tau')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "We will run over a few years and calibrate with a warmup of two years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Assign simulation inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_ids = ms.get_subarea_ids()\n",
    "rainfall_ids = mk_full_data_id('subarea', sa_ids, \"P\")\n",
    "evap_ids = mk_full_data_id('subarea', sa_ids, \"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.get_state_value(rainfall_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rids in rainfall_ids:\n",
    "    ms.play_input(daily_rain, rids)\n",
    "for evids in evap_ids:\n",
    "    ms.play_input(daily_pet, evids)\n",
    "ms.set_simulation_time_step('daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Define a calibration time span\n",
    "\n",
    "We define a calibration with objective calculation over 10 years, plus a 2-year warmup period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "w = e - pd.DateOffset(years=10)\n",
    "s = w - pd.DateOffset(years=2)\n",
    "\n",
    "print(f\"Calibration run: simulation from {s} to {e}, with a warmup till {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The package includes a function that flags possible inconsistencies prior to running a model (inconsistent time steps, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.check_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "We need to adjust a couple of parameters for proper operation on hourly data for the GR4 model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Wait what? The message is admitedly not the clearest, but in this case, we have not yet set a the simulation time span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.set_simulation_span(s, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Now, the check can compare simulation span and time series spans, and finds no problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.check_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## GR4J (GR4H) modes\n",
    "\n",
    "GR4J (j for \"journalier\" i.e. \"daily\") and GR4H (h for hourly) differ by the values of tso parameters. There are two helper functions to switch modes on all GR4J models in the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_hourly_gr4j(ms)\n",
    "ms.get_state_value(\"subarea.lnk1.UHExponent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_daily_gr4j(ms)\n",
    "ms.get_state_value(\"subarea.lnk1.UHExponent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Feasible parameter space and parsimony\n",
    "\n",
    "We have a catchment with 5 subareas, with GR4J. Leaving outside the links which we will not calibrate, this still means 20 parameters overall to calibrate. This can be problematic as there is likely an inflated parameter equifinality (many different combinations leading to sensibly similar performances), and the resulting parameters may not be robust or physically sensible.\n",
    "\n",
    "Instead, we can define a meta parameter set with only 4 degrees of freedom, with area scaling applied to x4 and time scaling applied to x2 and x3. The time scaling makes it invariant if the simulation time step changes from daily to hourly, but in this sample the most telling scaling is the one for the \"lag parameter\" `x4`. A single `x4` meta-parameter is reflected in each subarea with values than are scaled according to a function (square root) of the unit's area. Intuitively, it makes sense that the bigger the subarea, the longer the flow routing lag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_area = 250 # The area for which the scaling of x4 is invariant\n",
    "time_span = 86400 # The time step of the simulation, one day is 86400 seconds\n",
    "# time_span = 3600 # if we had an hourly simulation, and hourly inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "While it is possible to construct meta-parameterisers from scratch, it is tedious. The GR4J/H scaling strategy is well known and pre-implemented in `gr4j_scaled_parameteriser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gr4j_scaled_parameteriser(ref_area, time_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.as_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# set x4 bounds to be in \"days\", not hours\n",
    "p_x4 = pd.DataFrame.from_dict({\n",
    "    \"Name\": [\"x4\"],\n",
    "    \"Value\": [1.0],\n",
    "    \"Min\": [0.25],\n",
    "    \"Max\": [10.0],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_hypercube(p_x4)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "subarea_ids = paste(\"subarea\", get_subarea_ids(ms), sep=\".\")\n",
    "areas = get_state_value(ms, paste(subarea_ids, \"areaKm2\", sep=\".\"))\n",
    "areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Let us have a look at the values of the `x4` parameters in each subarea, before and after applying this meta-parameteriser `p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "x4_param_ids = paste(subarea_ids, \"x4\", sep=\".\")\n",
    "get_state_value(ms, x4_param_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.apply_sys_config(ms)\n",
    "get_state_value(ms, x4_param_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "The values of the individual x4 parameters are scaled according to the area of subareas. The larger the catchment, the larger the routing delay, the larger `x4`. The reference area for which the scaling would be 1.0 is 250 km^2, so the closer the catchment area from 250 km^2, the closer to 1.0 the area based scaling.\n",
    "\n",
    "We can compose a parameter transformation, on top of the tied. It is typical to calibrate on log(x4) rather than x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.wrap_transform()\n",
    "p.add_transform(\"log_x4\", \"x4\", \"log10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "outflowVarname = \"Catchment.StreamflowRate\"\n",
    "ms.record_state(outflowVarname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ms.exec_simulation()\n",
    "calc = ms.get_recorded(outflowVarname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "flow = as_xarray_series(daily_cumecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_e = as_timestamp(xr_ts_end(flow))\n",
    "vis_s = vis_e - pd.DateOffset(years=3)\n",
    "\n",
    "plot_two_series(flow, calc, names=[\"Observed\", \"Calculated\"], start_time=vis_s, end_time=vis_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Optimiser\n",
    "\n",
    "Let'c create an NSE evaluator, and check what the default parameter set yields as a goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = ms.create_objective(outflowVarname, flow, \"NSE\", w, e)\n",
    "score = objective.get_score(p)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "We have our objectives defined, and the parameter space 'p' in which to search. Let's create an optimiser and we are ready to go. While the optimiser can be created in one line, we show how to choose one custom termination criterion and how to configure the optimiser to capture a detailed log of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SWIFT_FULL\" in os.environ.keys():\n",
    "    max_hours = 0.2\n",
    "else:\n",
    "    max_hours = 0.02\n",
    "\n",
    "term = get_marginal_termination(tolerance = 1e-05, cutoff_no_improvement = 30, max_hours = max_hours)\n",
    "# term = get_max_runtime_termination(max_hours=max_hours)\n",
    "sce_params = get_default_sce_parameters()\n",
    "urs = create_parameter_sampler(0, p, \"urs\")\n",
    "optimiser = create_sce_optim_swift(objective, term, sce_params, urs)\n",
    "calib_logger = set_calibration_logger(optimiser, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "At this point you may want to specify the maximum number of cores that can be used by the optimiser, for instance if you wish to keep one core free to work in parallel on something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sce_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "The number of complexes is 6; by default the optimiser will try to use 6 CPU cores in parallel, or n-1 where N is your number of cores and less than 6. It is possible to limit the level of parallelism if needed, for instance to make sure you have a few cores to work with if an optimiser will run for some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser.set_maximum_threads_free_cores(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "calib_results = optimiser.execute_optimisation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "Processing the calibration log below. We subset the full log to keep only some types of optimiser messages, in this case we do not keep the \"shuffling\" stages of the SCE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_log = optimiser.extract_optimisation_log(fitness_name=\"NSE\")\n",
    "geom_ops = opt_log.subset_by_message(pattern=\"Initial.*|Reflec.*|Contrac.*|Add.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "We can then visualize how the calibration evolved. There are several types of visualisations included in the **mhplot** package, and numerous customizations possible, but starting with the overall population evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_ops._data[\"NSE\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_var_ids = p.as_dataframe().Name.values\n",
    "p_var_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "v = OptimisationPlots(geom_ops)\n",
    "for pVarId in p_var_ids:\n",
    "    v.parameter_evolution(pVarId, obj_lims=[0, 1])\n",
    "    plt.gcf().set_size_inches(10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortedResults = sortByScore(calib_results, 'NSE')\n",
    "# best_pset = getScoreAtIndex(sortedResults, 1)\n",
    "# best_pset = GetSystemConfigurationWila_R(best_pset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pset = calib_results.get_best_score(\"NSE\").parameteriser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "*swift* can back-transform a parameters to obtain the untransformed parameter set(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "untfPset = best_pset.backtransform()\n",
    "score = objective.get_score(best_pset)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = objective.get_score(untfPset)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Finally, let's have a visual of the fitted streamflow data at Bright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pset.apply_sys_config(ms)\n",
    "ms.exec_simulation()\n",
    "mod_runoff = ms.get_recorded(outflowVarname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plot_two_series(\n",
    "    flow, mod_runoff, start_time=vis_s, end_time=vis_e, names=[\"observed\", \"modelled\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# runoff = flow / sum(areasKm2)\n",
    "# runoff.plot()\n",
    "\n",
    "# plot_two_series(\n",
    "#     rainfall, runoff, start_time=vis_s, end_time=vis_e, names=[\"observed rain\", \"observed runoff\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,eval,name,-all",
   "formats": "ipynb,py:percent",
   "main_language": "R"
  },
  "kernelspec": {
   "display_name": "hydrofc",
   "language": "python",
   "name": "hydrofc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
