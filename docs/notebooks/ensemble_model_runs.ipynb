{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Ensemble SWIFT model runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## About this document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from swift2.doc_helper import pkg_versions_info\n",
    "\n",
    "print(pkg_versions_info(\"This document was generated from a jupyter notebook\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import swift2.doc_helper as std\n",
    "import swift2.parameteriser as sp\n",
    "import swift2.play_record as spr\n",
    "import uchronia.sample_data as usd\n",
    "import xarray as xr\n",
    "\n",
    "from swift2.const import CATCHMENT_FLOWRATE_VARID\n",
    "from swift2.simulation import get_subarea_ids\n",
    "from swift2.utils import mk_full_data_id, paste0\n",
    "from uchronia.data_set import datasets_summaries, get_dataset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Synthetic catchment\n",
    "\n",
    "Let's create a test catchment with a few subareas. Since we will work in a simulation mode, not calibration, we can afford a fairly arbitrary structure. \n",
    "\n",
    "A catchment structure can be captured with a set of items about subareas, links and nodes, and the connectivity of links and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_model='GR4J'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids=paste0('n', [i+1 for i in range(6)])\n",
    "link_ids = paste0('lnk', [i+1 for i in range(5)])\n",
    "node_names = paste0(node_ids, '_name')\n",
    "link_names = paste0(link_ids, '_name')\n",
    "from_node = paste0('n', [2,5,4,3,1])\n",
    "to_node = paste0('n', [6,2,2,4,4])\n",
    "areas_km2 = np.array([1.2, 2.3, 4.4, 2.2, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation = std.create_catchment(node_ids, node_names, link_ids, link_names, from_node, to_node, runoff_model, areas_km2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Input data management\n",
    "\n",
    "Working with ensemble time series is complicated.\n",
    "\n",
    "The package `uchronia` includes facilities to access time series from a \"library\", akin to what you would do to manage books. This hides a lot of the lower level code for reading and writing file. To an extent, the python package `xarray` overlaps with the features of these `uchronia` data libraries, but do not fully supersede them.\n",
    "\n",
    "Let's load a predefined data library with data for the Upper Murray river."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'SWIFT_TEST_DIR' in os.environ:\n",
    "    os.environ['SWIFT_TEST_DIR'] = os.path.expanduser('~/data/documentation') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_data_path = usd.sample_data_dir()\n",
    "data_path = os.path.join(doc_data_path, 'UpperMurray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_library = usd.sample_time_series_library('upper murray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = data_library.get_dataset_ids()\n",
    "data_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_library.datasets_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The sample catchment structure is obviously not the real \"Upper Murray\". For the sake of a didactic example, let's set the same inputs across all the subareas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_obs = data_library.get_dataset('rain_obs').to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rain_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_obs = rain_obs.where(rain_obs >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_obs.time[:-10].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Note that the rainfall is hourly, but the pet is daily. This will matter later for the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_obs = data_library.get_dataset('pet_obs').to_xarray()\n",
    "pet_obs.time[:-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Setting simulation inputs by reading from a data library\n",
    "\n",
    "swift simulations have a `play_inputs` (with an s) method designed to retrieve inputs from a library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.play_inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "`play_inputs` can accept vectorised arguments, which is handy for cases like the following:\n",
    "\n",
    "> For each precipitation model input, use the same time series 'rain_obs' from the data library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_ids = mk_full_data_id( 'subarea', get_subarea_ids(simulation), 'P')\n",
    "evapIds = mk_full_data_id( 'subarea', get_subarea_ids(simulation), 'E')\n",
    "precip_ids, evapIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rep(x): return np.repeat(x, len(precip_ids))\n",
    "simulation.play_inputs(data_library, precip_ids, _rep('rain_obs'), _rep(''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "We noted that the pet_obs is a daily series, not hourly as per . `swift2` can disaggregate on the fly, using the 'daily_to_hourly' method when assigning inputs to the simulation. This saves a lot of tedium!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.play_inputs(data_library, evapIds, _rep('pet_obs'), _rep('daily_to_hourly'))\n",
    "# And the flow rate we will record\n",
    "outflow_id = CATCHMENT_FLOWRATE_VARID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Given the information from the input data, let's define a suitable simulation time span. We have define an ensemble simulation where we will do a warmup simulation on a single input (no \"ensemble\") for 3 years or so, then five days of an ensemble simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cinterop.timeseries import as_timestamp\n",
    " \n",
    "s = as_timestamp('2007-01-01')\n",
    "e = as_timestamp('2010-08-01 20')\n",
    "s_hot = as_timestamp('2010-08-01 21')\n",
    "e_hot = as_timestamp('2010-08-05 21')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Warmup the simulation to get 'hot' states\n",
    "\n",
    "First, before demonstrating ensemble forecasting simulations, let's demonstrate how we can get a snapshot of the model states at a point in time and restore it later on, hot-starting further simulation.\n",
    "\n",
    "We deliberately get into details here to illustrate how to capture states, and run simulation without or without state reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.set_simulation_span(start=s, end=e_hot)\n",
    "simulation.record_state(outflow_id)\n",
    "simulation.exec_simulation()\n",
    "baseline = simulation.get_recorded(outflow_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.squeeze(drop=True).sel(time = slice(s_hot, e_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.plot(figsize=(10,5))\n",
    "plt.title(\"streamflow with long term simulation, slice to the end of the series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.set_simulation_span(start=s, end=e)\n",
    "simulation.exec_simulation()\n",
    "snapshot = simulation.snapshot_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "We can execute a simulation over the new time span, but requesting model states to NOT be reset. If we compare with a simulation where, as per default, the states are reset before the first time step, we notice a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.set_simulation_span(start=s_hot, end=e_hot)\n",
    "simulation.exec_simulation(reset_initial_states = False)\n",
    "noReset = simulation.get_recorded(outflow_id)\n",
    "simulation.exec_simulation(reset_initial_states = True)\n",
    "withReset = simulation.get_recorded(outflow_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "noReset = noReset.squeeze(drop=True)\n",
    "\n",
    "x = xr.concat([noReset,withReset], dim=pd.Index(['no reset','reset'], name='scenario')).squeeze(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(x.time.values, x.sel(scenario='no reset'), linewidth=2, label='No reset')\n",
    "ax.plot(x.time.values, x.sel(scenario='reset'), linewidth=2, label='Reset')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "The simulation hot-started and run with no reset is like the previous long simulation baseline. If we reset the states to zero, we even have ho streamflow yet produced over these 5 days..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Ensemble forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Now let'd ready the simulation to do ensemble forecasts. We define a list `inputMap` such that keys are the names of ensemble forecast time series found in `data_library` and the values is one or more of the model properties found in the simulation. In this instance we use the same series for all model precipitation inputs in `precip_ids` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.reset_model_states()\n",
    "simulation.set_states(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputMap = {'rain_fcast_ens':precip_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ems = simulation.create_ensemble_forecast_simulation(\n",
    "    data_library, \n",
    "    start=s_hot, \n",
    "    end=e_hot, \n",
    "    input_map=inputMap, \n",
    "    lead_time=(24*2+23), \n",
    "    ensemble_size=100, \n",
    "    n_time_steps_between_forecasts=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "`ems` is an ensemble forecast simulation object, which is an augmentation of the `Simulation` object that deals with non-ensemble simulation. It is very important to note that whenever possible, the object methods are named identically, just that the time series in and out of the simulations are of higher dimension.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ems.get_simulation_span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ems.record_state(outflow_id)\n",
    "ems.exec_simulation()\n",
    "forecasts = ems.get_recorded_ensemble_forecast(outflow_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "We have four forecast issue times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts.time_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "We can retrieve the first forecast issues at '2010-08-01 21:00:00' by indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_forecasts = forecasts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Let's visualise each of these successive ensemble forecasts. We define a function to determine and visualise the quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ensemble_forecast(flow_forecasts, issue_date):\n",
    "    q = flow_forecasts.quantile([0.05, .25, .5, .75, 0.95], 'ensemble')\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    ax.fill_between(q.time.values, q.sel(quantile=0.05), q.sel(quantile=0.95), alpha=0.3, label='Perc. 50-95')\n",
    "    ax.fill_between(q.time.values, q.sel(quantile=0.25), q.sel(quantile=.75), alpha=0.5, label='Perc. 25-75')\n",
    "    ax._get_lines.get_next_color()  # Hack to get different line\n",
    "    ax.plot(q.time.values, q.sel(quantile=.5), linewidth=2, label='Median')\n",
    "    ax.legend()\n",
    "    dd = pd.Timestamp(issue_date).strftime('%Y-%m-%dT%H')\n",
    "    plt.title(f\"Ensemble forecast streamflow {dd}\")\n",
    "    plt.ylabel(\"Streamflow (m3/s)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_dates = forecasts.time_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ensemble_forecast(forecasts[0], issue_dates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ensemble_forecast(forecasts[1], issue_dates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ensemble_forecast(forecasts[2], issue_dates[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ensemble_forecast(forecasts[3], issue_dates[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Data library sample definition\n",
    "\n",
    "The sample data library used in this vignette is defined by a YAML file defining where time series (or ensemble time series) are on disk in netcdf files. Note that one series can be in several netCDF files, and conversely it is possible to define multiple series (e.g. per station) in one file.\n",
    "\n",
    "```yaml\n",
    "pet_fcast_ens:\n",
    "  Type: single\n",
    "  Id: pet_fcast_ens\n",
    "  Storage:\n",
    "    Type: single_nc_file\n",
    "    File: ./Fct_Data/Upper_Murray_pet_clim_1990_2010.nc\n",
    "    Identifier: 1\n",
    "    Var: pet_der\n",
    "pet_obs:\n",
    "  Type: single\n",
    "  Id: pet_obs\n",
    "  Storage:\n",
    "    Type: single_nc_file\n",
    "    File: ./Obs_data/Upper_Murray_pet_24h_89_2012.nc\n",
    "    Identifier: 1\n",
    "    Var: pet_der\n",
    "rain_obs:\n",
    "  Type: single\n",
    "  Id: rain_obs\n",
    "  Storage:\n",
    "    Type: single_nc_file\n",
    "    File: ./Obs_data/Upper_Murray_rain_1hr.nc\n",
    "    Identifier: 1\n",
    "    Var: rain_der\n",
    "rain_fcast_ens:\n",
    "  Type: ts_ensemble_ts\n",
    "  Id: rain_fcast_ens\n",
    "  # The following 6 values are placeholders - may not yet be used by time series library\n",
    "  TimeStep: 24:00:00\n",
    "  Start: 2010-08-01T21:00:00\n",
    "  Length: 5\n",
    "  EnsembleSize: 1000\n",
    "  EnsembleLength: 240\n",
    "  EnsembleTimeStep: 01:00:00\n",
    "  Storage:\n",
    "    Type: multiple_nc_files_filename_date_pattern\n",
    "    File: ./Fct_Data/Upper_Murray_F1_1_{0}21_shuffle.nc\n",
    "    Var: rain_fcast_ens\n",
    "    Identifier: 1\n",
    "    Index: 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,eval,name,-all",
   "formats": "ipynb,py:percent",
   "main_language": "R"
  },
  "kernelspec": {
   "display_name": "hydrofc",
   "language": "python",
   "name": "hydrofc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
